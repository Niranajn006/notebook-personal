# Mon 04 Dec 2017

**Reading large data files into Pandas**

Spark is very good at reading large amounts of data - spread across many files - from s3 into a dataframe and the dataframe class has a `.toPandas()` method that converts a spark dataframe into a pandas one.

Also, `split` the linux utility is very handy for splitting files up: https://linux.die.net/man/1/split

Dask is very good at reading large csvs, or csvs spread across a lot of files, and it is more lightweight than spark: http://dask.pydata.org/en/latest/dataframe-create.html

## Links

### Software Engineering

- [Docker inspect template magic](http://container-solutions.com/docker-inspect-template-magic/)
  - `format` takes Go templates
  - useful introduction to the template logic that can be used

[DeepMind papers at NIPS 2017](https://deepmind.com/blog/deepmind-papers-nips-2017/)

[AI Index: Latest curated data and trends on artificial intelligence in one place](http://www.aiindex.org/)

[Easiest way to install fastboot and adb on any OS](https://lifehacker.com/the-easiest-way-to-install-androids-adb-and-fastboot-to-1586992378/amp)

[Mount a USB drive in a Docker container](https://docs.cancergenomicscloud.org/docs/mount-a-usb-drive-in-a-docker-container)

[Relocking Xiaomi bootloader](http://en.miui.com/thread-279195-1-1.html)

[YouTube: how to relock Xiaomi bootloader (video)](https://m.youtube.com/watch?v=Gf6_hbdtECE)